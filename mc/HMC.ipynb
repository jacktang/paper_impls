{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hamiltonian Monte Carlo (HMC) is an extension of Metropolis-Hastings(MH).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm breaks down into four parts:\n",
    "\n",
    "1. Set up: Take the previous position and copy, such that you have q0 and q1. Randomly sample a momentum from N(0,1) and copy, such that you have p0 and p1. Find the gradient of the PDF with respect to position -(x-mu)/sigma^2 for a single variable gaussian.\n",
    "2. Leapfrog: Use leapfrog integration to update q1 and p1 (ie Hamiltonian motion or a particle.) In practice, this is the most sensitive part; small adjustments can induce unstable behavior.\n",
    "3. MH Dance: Lastly, multiply p1 by (-1), this ensures reversibility (ie q1 can be reached from q0 given momentum p0 AND q0 can be reached from q1 given momentum -p1). This gives us the information we need for the metropolis-hastings “dance”. Keep in mind we’re using negative log probabilities, so the math is all in terms of addition and subtraction. Then accept/reject movement.\n",
    "4. Repeat for fixed number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normal(x,mu,sigma):\n",
    "    numerator = np.exp(-1*((x-mu)**2)/(2*sigma**2))\n",
    "    denominator = sigma * np.sqrt(2*np.pi)\n",
    "    return numerator/denominator\n",
    "\n",
    "def neg_log_prob(x,mu,sigma):\n",
    "    return -1*np.log(normal(x=x,mu=mu,sigma=sigma))\n",
    "\n",
    "def HMC(mu=0.0,sigma=1.0,path_len=1,step_size=0.25,initial_position=0.0,epochs=1_000):\n",
    "    # setup\n",
    "    steps = int(path_len/step_size) # path_len and step_size are tricky parameters to tune...\n",
    "    samples = [initial_position]\n",
    "    momentum_dist = st.norm(0, 1) \n",
    "    # generate samples\n",
    "    for e in range(epochs):\n",
    "        q0 = np.copy(samples[-1])\n",
    "        q1 = np.copy(q0)\n",
    "        p0 = momentum_dist.rvs()        \n",
    "        p1 = np.copy(p0) \n",
    "        dVdQ = -1*(q0-mu)/(sigma**2) # gradient of PDF wrt position (q0) aka potential energy wrt position\n",
    "\n",
    "        # leapfrog integration begin\n",
    "        for s in range(steps): \n",
    "            p1 += step_size*dVdQ/2 # as potential energy increases, kinetic energy decreases, half-step\n",
    "            q1 += step_size*p1 # position increases as function of momentum \n",
    "            p1 += step_size*dVdQ/2 # second half-step \"leapfrog\" update to momentum    \n",
    "        # leapfrog integration end        \n",
    "        p1 = -1*p1 #flip momentum for reversibility     \n",
    "\n",
    "        \n",
    "        #metropolis acceptance\n",
    "        q0_nlp = neg_log_prob(x=q0,mu=mu,sigma=sigma)\n",
    "        q1_nlp = neg_log_prob(x=q1,mu=mu,sigma=sigma)        \n",
    "\n",
    "        p0_nlp = neg_log_prob(x=p0,mu=0,sigma=1)\n",
    "        p1_nlp = neg_log_prob(x=p1,mu=0,sigma=1)\n",
    "        \n",
    "        # Account for negatives AND log(probabiltiies)...\n",
    "        target = q0_nlp - q1_nlp # P(q1)/P(q0)\n",
    "        adjustment = p1_nlp - p0_nlp # P(p1)/P(p0)\n",
    "        acceptance = target + adjustment \n",
    "        \n",
    "        event = np.log(random.uniform(0,1))\n",
    "        if event <= acceptance:\n",
    "            samples.append(q1)\n",
    "        else:\n",
    "            samples.append(q0)\n",
    "    \n",
    "    return samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mu = 0\n",
    "sigma = 1\n",
    "trial = HMC(mu=mu,sigma=sigma,path_len=1.5,step_size=0.25)\n",
    "\n",
    "lines = np.linspace(-6,6,10_000)\n",
    "normal_curve = [normal(x=l,mu=mu,sigma=sigma) for l in lines]\n",
    "\n",
    "plt.plot(lines,normal_curve)\n",
    "plt.hist(trial,density=True,bins=20)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* https://towardsdatascience.com/python-hamiltonian-monte-carlo-from-scratch-955dba96a42d\n",
    "* Simulation-based Probabilistic Risk Assessment : https://arxiv.org/pdf/2207.12575.pdf\n",
    "* https://zhijingeu.medium.com/building-a-probabilistic-risk-estimate-using-monte-carlo-simulations-with-python-mcerp-7d57e63112fa\n",
    "* https://towardsdatascience.com/python-hamiltonian-monte-carlo-from-scratch-955dba96a42d"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
