{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian data analysis! Let me break it down with a simple example.\n",
      "\n",
      "**What is Bayesian data analysis?**\n",
      "\n",
      "Bayesian data analysis is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis based on new data. It's a way to quantify uncertainty and make inferences about a population or phenomenon using data and prior knowledge.\n",
      "\n",
      "**A Simple Example: Coin Flipping**\n",
      "\n",
      "Imagine you have a coin that you suspect is biased, meaning it lands on heads more often than tails. You want to know the probability of getting heads when you flip the coin.\n",
      "\n",
      "**Prior Knowledge**\n",
      "\n",
      "Before collecting any data, you have some prior knowledge about the coin. You think the probability of getting heads is around 0.5, but you're not sure. You represent your prior knowledge as a probability distribution, say, a Beta distribution with parameters α = 1 and β = 1, which is a uniform distribution between 0 and 1. This prior distribution reflects your uncertainty about the true probability of getting heads.\n",
      "\n",
      "**Data Collection**\n",
      "\n",
      "You flip the coin 10 times and get the following results:\n",
      "\n",
      "Heads: 7\n",
      "Tails: 3\n",
      "\n",
      "**Likelihood**\n",
      "\n",
      "Now, you want to update your prior knowledge with the new data. You calculate the likelihood of observing the data given the probability of getting heads (θ). In this case, the likelihood is a Binomial distribution with parameters n = 10 and p = θ.\n",
      "\n",
      "**Posterior Distribution**\n",
      "\n",
      "Next, you update your prior distribution with the likelihood to get the posterior distribution, which represents your updated knowledge about the probability of getting heads. Using Bayes' theorem, you multiply the prior distribution with the likelihood and normalize the result to obtain the posterior distribution.\n",
      "\n",
      "**Posterior Distribution: Beta(8, 4)**\n",
      "\n",
      "The updated posterior distribution is a Beta distribution with parameters α = 8 and β = 4. This distribution reflects your updated uncertainty about the true probability of getting heads.\n",
      "\n",
      "**Inferences**\n",
      "\n",
      "From the posterior distribution, you can make inferences about the probability of getting heads. For example, the mean of the posterior distribution is approximately 0.67, which indicates that the probability of getting heads is likely around 0.67. You can also calculate credible intervals or make predictions about future coin flips.\n",
      "\n",
      "**Key Takeaways**\n",
      "\n",
      "* Bayesian data analysis combines prior knowledge with new data to update our uncertainty about a parameter of interest.\n",
      "* The posterior distribution represents our updated knowledge about the parameter, taking into account the data and prior knowledge.\n",
      "* This approach allows for probabilistic inference and uncertainty quantification, which is useful in many applications, such as decision-making, forecasting, and hypothesis testing.\n",
      "\n",
      "I hope this simple example helps illustrate the basics of Bayesian data analysis!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "os.environ['http_proxy'] = \"http://10.0.5.8:30890\" \n",
    "os.environ['https_proxy'] = \"http://10.0.5.8:30890\" \n",
    "\n",
    "client = Groq(\n",
    "    # This is the default and can be omitted\n",
    "    api_key= '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' #os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the bayesian data analysis with simple example\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Low-latency Large Language Models (LLMs) are crucial in various applications where real-time or near-real-time processing is essential. Here are some reasons why low-latency LLMs are important:\\n\\n1. **Conversational AI**: In conversational AI, low-latency LLMs enable more natural and human-like interactions. Fast response times are critical in chatbots, virtual assistants, and voice assistants to maintain user engagement and provide a seamless experience.\\n2. **Real-time Language Translation**: Low-latency LLMs facilitate instant language translation, which is vital in applications like live subtitles, real-time translation for international conferences, or simultaneous interpretation for diplomatic meetings.\\n3. **Sentiment Analysis and Feedback**: In customer service, low-latency LLMs can quickly analyze customer feedback, enabling companies to respond promptly to concerns and improve customer satisfaction.\\n4. **Content Generation and Summarization**: Fast LLMs can generate content, such as news summaries or product descriptions, in real-time, allowing for timely and relevant information dissemination.\\n5. **Healthcare and Emergency Services**: In healthcare, low-latency LLMs can rapidly analyze medical texts, enabling doctors to make timely decisions. In emergency services, fast language processing can help dispatchers quickly understand and respond to emergency calls.\\n6. **Gaming and Interactive Systems**: Low-latency LLMs can enhance gaming experiences by enabling real-time dialogue systems, interactive storytelling, and responsive NPCs (non-player characters).\\n7. **Autonomous Systems and Robotics**: In autonomous systems, fast LLMs can process and respond to voice commands, enabling more efficient human-robot interaction and improving overall system performance.\\n8. **Cybersecurity and Threat Detection**: Low-latency LLMs can quickly analyze network traffic, detect threats, and respond to cyber attacks in real-time, reducing the risk of security breaches.\\n9. **Accessibility and Inclusive Design**: Fast LLMs can provide real-time transcriptions, enabling people with disabilities to engage more easily with digital content and improving overall accessibility.\\n10. **Competitive Advantage**: In many industries, low-latency LLMs can provide a competitive edge by enabling faster decision-making, improved customer experiences, and increased operational efficiency.\\n\\nIn summary, low-latency LLMs are essential in various applications where timely processing and response are critical. They can improve user experiences, enhance decision-making, and provide a competitive advantage in many industries.', response_metadata={'token_usage': {'completion_time': 1.635, 'completion_tokens': 489, 'prompt_time': 0.017, 'prompt_tokens': 32, 'queue_time': None, 'total_time': 1.652, 'total_tokens': 521}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2e673c5-95ef-48d1-a923-2ffda99ee679-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "api_key = \"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\"\n",
    "chat = ChatGroq(temperature=0, groq_api_key=api_key, model_name=\"llama3-70b-8192\")\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silvery glow bright\n",
      "Luna's gentle light shines down\n",
      "Midnight's gentle queen"
     ]
    }
   ],
   "source": [
    "chat = ChatGroq(temperature=0, groq_api_key=api_key, model_name=\"llama3-70b-8192\")\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a haiku about {topic}\")])\n",
    "chain = prompt | chat\n",
    "for chunk in chain.stream({\"topic\": \"The Moon\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
