{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA(Lower Rank Adaption) basic\n",
    "\n",
    "As the original LoRA paper outlines, LoRA introduces an additional scaling coefficient for applying the LoRA weights to the pretrained weights during the forward pass. The scaling involves the rank parameter r, which we discussed earlier, as well as another hyperparameter Î± (alpha) that is applied as follows:\n",
    "\n",
    "scaling = alpha / r\n",
    "weight += (lora_B @ lora_A) * scaling \n",
    "\n",
    "\n",
    "## What's LoRA and the applications\n",
    "\n",
    "## QLoRA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "* https://lightning.ai/pages/community/lora-insights/\n",
    "* https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\n",
    "* https://www.entrypointai.com/blog/lora-fine-tuning/\n",
    "* Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments: https://lightning.ai/pages/community/lora-insights/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
