{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "## ReLU\n",
    "\n",
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# create a PyTorch tensor\n",
    "x = torch.linspace(-10, 10, 100)\n",
    " \n",
    "# apply the logistic activation function to the tensor\n",
    "y = torch.sigmoid(x)\n",
    " \n",
    "# plot the results with a custom color\n",
    "plt.plot(x.numpy(), y.numpy(), color='purple')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Logistic Activation Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark: https://arxiv.org/abs/2109.14545\n",
    "* https://machinelearningmastery.com/activation-functions-in-pytorch/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
